<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PUBLICATIONS | Slim Essid's research page </title> <meta name="author" content="Slim Essid"> <meta name="description" content="Slim Essid's research activities "> <meta name="keywords" content="AI, artificial intelligence, machine learning, deep learning, signal processing, audio, multimodal data, multimodal machine preception, multiview learning, representation learning, self supervised learning, machine listening, speech processing, source separation, DCASE"> <link rel="stylesheet" href="/research/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/research/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/research/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%92%A1&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/research/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://slimessid.github.io/research/publications/"> <link defer rel="stylesheet" href="/research/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/research/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/research//"> Slim Essid's research page </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/research/">ABOUT </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/publications/">PUBLICATIONS <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/people/">PEOPLE </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/projects/">PROJECTS </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/teaching/">TEACHING </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">PUBLICATIONS</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="quelennec:hal-04360221" class="col-sm-8"> <div class="title">ON THE CHOICE OF THE OPTIMAL TEMPORAL SUPPORT FOR AUDIO CLASSIFICATION WITH PRE-TRAINED EMBEDDINGS</div> <div class="author"> A. Quelennec , M. Olvera , G. Peeters , and <em>S. Essid</em> </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)</em> , Apr 2024 </div> <div class="periodical"> Accepted </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.science/hal-04360221/file/Pre_print_ICASSP_Paper.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Current state-of-the-art audio analysis systems rely on pre-trained embedding models, often used off-the-shelf as (frozen) feature extractors. Choosing the best one for a set of tasks is the subject of many recent publications. However, one aspect often overlooked in these works is the influence of the duration of audio input considered to extract an embedding, which we refer to as Temporal Support (TS). In this work, we study the influence of the TS for well-established or emerging pre-trained embeddings, chosen to represent different types of architectures and learning paradigms. We conduct this evaluation using both musical instrument and environmental sound datasets, namely OpenMIC, TAU Urban Acoustic Scenes 2020 Mobile, and ESC-50. We especially highlight that Audio Spectrogram Transformer-based systems (PaSST and BEATs) remain effective with smaller TS, which therefore allows for a drastic reduction in memory and computational cost. Moreover, we show that by choosing the optimal TS we reach competitive results across all tasks. In particular, we improve the state-of-the-art results on OpenMIC, using BEATs and PaSST without any fine-tuning. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="gruttadauria:hal-04419041" class="col-sm-8"> <div class="title">ONLINE SPEAKER DIARIZATION OF MEETINGS GUIDED BY SPEECH SEPARATION</div> <div class="author"> E. Gruttadauria , M. Fontaine , and <em>S. Essid</em> </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2024)</em> , Apr 2024 </div> <div class="periodical"> Accepted </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.science/hal-04419041/file/ICASSP_2024_ELIO_GRUTTADAURIA-final.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Overlapped speech is notoriously problematic for speaker diarization systems. Consequently, the use of speech separation has recently been proposed to improve their performance. Although promising, speech separation models struggle with realistic data because they are trained on simulated mixtures with a fixed number of speakers. In this work, we introduce a new speech separation-guided diarization scheme suitable for the online speaker diarization of long meeting recordings with a variable number of speakers, as present in the AMI corpus. We envisage ConvTasNet and DPRNN as alternatives for the separation networks, with two or three output sources. To obtain the speaker diarization result, voice activity detection is applied on each estimated source. The final model is fine-tuned end-to-end, after first adapting the separation to real data using AMI. The system operates on short segments, and inference is performed by stitching the local predictions using speaker embeddings and incremental clustering. The results show that our system improves the state-of-the-art on the AMI headset mix, using no oracle information and under full evaluation (no collar and including overlapped speech). Finally, we show the strength of our system particularly on overlapped speech sections. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="buisson:ismir23" class="col-sm-8"> <div class="title">A REPETITION-BASED TRIPLET MINING APPROACH FOR MUSIC SEGMENTATION</div> <div class="author"> M. Buisson , B. McFee , <em>S. Essid</em>, and H. Crayencour </div> <div class="periodical"> <em>In International Society for Music Information Retrieval (ISMIR)</em> , Nov 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.science/hal-04202766/file/A%20Repetition-Based%20Triplet%20Mining%20Approach%20for%20Music%20Segmentation.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Contrastive learning has recently appeared as a well-suited method to find representations of music audio signals that are suitable for structural segmentation. However, most existing unsupervised training strategies omit the notion of repetition and therefore fail at encompassing this essential aspect of music structure. This work introduces a triplet mining method which explicitly considers repeating sequences occurring inside a music track by leveraging common audio descriptors. We study its impact on the learned representations through downstream music segmentation. Because musical repetitions can be of different natures, we give further insight on the role of the audio descriptors employed at the triplet mining stage as well as the trade-off existing between the quality of the triplets mined and the quantity of unlabelled data used for training. We observe that our method requires less non-annotated data while remaining competitive against other unsupervised methods trained on a larger corpus. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="letzelter:hal-04216055" class="col-sm-8"> <div class="title">RESILIENT MULTIPLE CHOICE LEARNING: A LEARNED SCORING SCHEME WITH APPLICATION TO AUDIO SCENE ANALYSIS</div> <div class="author"> V. Letzelter , M. Fontaine , P. Perez , G. Richard , <em>S. Essid</em>, and M. Chen </div> <div class="periodical"> <em>In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023)</em> , Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.science/hal-04216055/file/neurips_2023.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> We introduce Resilient Multiple Choice Learning (rMCL), an extension of the MCL approach for conditional distribution estimation in regression settings where multiple targets may be sampled for each training input. Multiple Choice Learning is a simple framework to tackle multimodal density estimation, using the Winner-Takes-All (WTA) loss for a set of hypotheses. In regression settings, the existing MCL variants focus on merging the hypotheses, thereby eventually sacrificing the diversity of the predictions. In contrast, our method relies on a novel learned scoring scheme underpinned by a mathematical framework based on Voronoi tessellations of the output space, from which we can derive a probabilistic interpretation. After empirically validating rMCL with experiments on synthetic data, we further assess its merits on the sound source localization problem, demonstrating its practical usefulness and the relevance of its interpretation. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="zaiem:interspeech23" class="col-sm-8"> <div class="title">SPEECH SELF-SUPERVISED REPRESENTATION BENCHMARKING: ARE WE DOING IT RIGHT?</div> <div class="author"> S. Zaiem , T. Parcollet , and <em>S. Essid</em> </div> <div class="periodical"> <em>In Interspeech</em> , Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hal.science/hal-04216175/file/zaiem23b_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="zaiem:interspeech23b" class="col-sm-8"> <div class="title">AUTOMATIC DATA AUGMENTATION FOR DOMAIN ADAPTED FINE-TUNING OF SELF-SUPERVISED SPEECH REPRESENTATIONS</div> <div class="author"> S. Zaiem , T. Parcollet , and <em>S. Essid</em> </div> <div class="periodical"> <em>In Interspeech</em> , Aug 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.science/hal-04216177/file/zaiem23_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Self-Supervised Learning (SSL) has allowed leveraging large amounts of unlabeled speech data to improve the performance of speech recognition models even with small annotated datasets. Despite this, speech SSL representations may fail while facing an acoustic mismatch between the pretraining and target datasets. To address this issue, we propose a novel supervised domain adaptation method, designed for cases exhibiting such a mismatch in acoustic domains. It consists in applying properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset. The approach is validated during an oracle experiment with controlled distortions and on two amateur-collected low-resource domains, reaching better performances compared to the baselines in both cases. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Benigmim_2023_CVPR" class="col-sm-8"> <div class="title">ONE-SHOT UNSUPERVISED DOMAIN ADAPTATION WITH PERSONALIZED DIFFUSION MODELS</div> <div class="author"> Y. Benigmim , S. Roy , <em>S. Essid</em>, V. Kalogeiton , and S. Lathuiliere </div> <div class="periodical"> <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</em> , Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p> Adapting a segmentation model from a labeled source domain to a target domain, where a single unlabeled datum is available, is one the most challenging problems in domain adaptation and is otherwise known as one-shot unsupervised domain adaptation (OSUDA). Most of the prior works have addressed the problem by relying on style transfer techniques, where the source images are stylized to have the appearance of the target domain. Departing from the common notion of transferring only the target “texture” information, we leverage text-to-image diffusion models (e.g., Stable Diffusion) to generate a synthetic target dataset with photo-realistic images that not only faithfully depict the style of the target domain, but are also characterized by novel scenes in diverse contexts. The text interface in our method Data AugmenTation with diffUsion Models (DATUM) endows us with the possibility of guiding the generation of images towards desired semantic concepts while respecting the original spatial context of a single training image, which is not possible in existing OSUDA methods. Extensive experiments on standard benchmarks show that our DATUM surpasses the state-of-the-art OSUDA methods by up to +7.1%. The implementation is available at https://github.com/yasserben/DATUM </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="10095833" class="col-sm-8"> <div class="title">COSMOPOLITE SOUND MONITORING (COSMO): A STUDY OF URBAN SOUND EVENT DETECTION SYSTEMS GENERALIZING TO MULTIPLE CITIES</div> <div class="author"> F. Angulo , <em>S. Essid</em>, G. Peeters , and C. Mietlicki </div> <div class="periodical"> <em>In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Jun 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="zaiem:hal-04076307" class="col-sm-8"> <div class="title">FINE-TUNING STRATEGIES FOR FASTER INFERENCE USING SPEECH SELF-SUPERVISED MODELS: A COMPARATIVE STUDY</div> <div class="author"> S. Zaiem , R. Algayres , T. Parcollet , <em>S. Essid</em>, and M. Ravanelli </div> <div class="periodical"> <em>In ICASSP 2023 - International Conference on Acoustics, Speech, and Signal Processing</em> , Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.science/hal-04076307/file/2303.06740.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Self-supervised learning (SSL) has allowed substantial progress in Automatic Speech Recognition (ASR) performance in low-resource settings. In this context, it has been demonstrated that larger selfsupervised feature extractors are crucial for achieving lower downstream ASR error rates. Thus, better performance might be sanctioned with longer inferences. This article explores different approaches that may be deployed during the fine-tuning to reduce the computations needed in the SSL encoder, leading to faster inferences. We adapt a number of existing techniques to common ASR settings and benchmark them, displaying performance drops and gains in inference times. Interestingly, we found that given enough downstream data, a simple downsampling of the input sequences outperforms the other methods with both low performance drops and high computational savings, reducing computations by 61.3% with an WER increase of only 0.81. Finally, we analyze the robustness of the comparison to changes in dataset conditions, revealing sensitivity to dataset size. </p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="perera:hal-03782827" class="col-sm-8"> <div class="title">LATENT AND ADVERSARIAL DATA AUGMENTATION FOR SOUND EVENT DETECTION AND CLASSIFICATION</div> <div class="author"> D. Perera , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In International workshop on Detection and Classiffication of Acoustic Scenes and Events (DCASE)</em> , Nov 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.science/hal-03782827/file/dcase.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Invariance-based learning is a promising approach in deep learning. Among other benefits, it can mitigate the lack of diversity of available datasets and increase the interpretability of trained models. To this end, practitioners often use a consistency cost penalizing the sensitivity of a model to a set of carefully selected data augmentations. However, there is no consensus about how these augmentations should be selected. In this paper, we study the behavior of several augmentation strategies. We consider the task of sound event detection and classification for our experiments. In particular, we show that transformations operating on the internal layers of a deep neural network are beneficial for this task. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="perera:hal-03759651" class="col-sm-8"> <div class="title">IMPACT DE PERTURBATIONS INTERNES SUR L ENTRAINEMENT DE RESEAUX PROFONDS POUR LA DETECTION D EVENEMENTS SONORES</div> <div class="author"> D. Perera , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In Colloque Francophone de Traitement du Signal et des Images (GRETSI)</em> , Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hal.telecom-paris.fr/hal-03759651/file/perera927.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="9846981" class="col-sm-8"> <div class="title">PRETEXT TASKS SELECTION FOR MULTITASK SELF-SUPERVISED AUDIO REPRESENTATION LEARNING</div> <div class="author"> S. Zaiem , T. Parcollet , <em>S. Essid</em>, and A. Heba </div> <div class="periodical"> <em>IEEE Journal of Selected Topics in Signal Processing</em>, Sep 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.science/hal-03601330/file/2107.00594.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Through solving pretext tasks, self-supervised learning leverages unlabeled data to extract useful latent representations replacing traditional input features in the downstream task. In audio/speech signal processing, a wide range of features where engineered through decades of research efforts. As it turns out, learning to predict such features (a.k.a pseudo-labels) has proven to be a particularly relevant pretext task, leading to useful self-supervised representations which prove to be effective for downstream tasks. However, methods and common practices for combining such pretext tasks for better performance on the downstream task have not been explored and understood properly. In fact, the process relies almost exclusively on a computationally heavy experimental procedure, which becomes intractable with the increase of the number of pretext tasks. This paper introduces a method to select a group of pretext tasks among a set of candidates. The method we propose estimates calibrated weights for the partial losses corresponding to the considered pretext tasks during the self-supervised training process. The experiments conducted on automatic speech recognition, speaker and emotion recognition validate our approach, as the groups selected and weighted with our method perform better than classic baselines, thus facilitating the selection and combination of relevant pseudo-labels for self-supervised representation learning. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="zaiem22_interspeech" class="col-sm-8"> <div class="title">AUTOMATIC DATA AUGMENTATION SELECTION AND PARAMETRIZATION IN CONTRASTIVE SELF-SUPERVISED SPEECH REPRESENTATION LEARNING</div> <div class="author"> S. Zaiem , T. Parcollet , and <em>S. Essid</em> </div> <div class="periodical"> <em>In Proc. Interspeech 2022</em> , Sep 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="buisson:hal-03780032" class="col-sm-8"> <div class="title">LEARNING MULTI-LEVEL REPRESENTATIONS FOR HIERARCHICAL MUSIC STRUCTURE ANALYSIS</div> <div class="author"> M. Buisson , B. McFee , <em>S. Essid</em>, and H. Crayencour </div> <div class="periodical"> <em>In International Society for Music Information Retrieval (ISMIR)</em> , Dec 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.science/hal-03780032/file/Morgan_Buisson_ismir.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> Recent work in music structure analysis has shown the potential of deep features to highlight the underlying structure of music audio signals. Despite promising results achieved by such representations, dealing with the inherent hierarchical aspect of music structure remains a challenging problem. Because different levels of segmentation can be considered as equally valid, specifically designed representations should be optimized to improve hierarchical structure analysis. In this work, unsupervised learning of such representations using a contrastive approach operating at different timescales is explored. The proposed system is evaluated on flat and multi-level music segmentation. By leveraging both time and the hierarchical organization of music structure, we show that the obtained deep embeddings can encode meaningful patterns and improve segmentation at various levels of granularity. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="barriere:hal-04276012" class="col-sm-8"> <div class="title">OPINIONS IN INTERACTIONS : NEW ANNOTATIONS OF THE SEMAINE DATABASE</div> <div class="author"> V. Barrière , C. Clavel , and <em>S. Essid</em> </div> <div class="periodical"> <em>In LREC</em> , Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hal.science/hal-04276012/file/2022.lrec-1.762.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="furnon:hal-02985867" class="col-sm-8"> <div class="title">DNN-BASED MASK ESTIMATION FOR DISTRIBUTED SPEECH ENHANCEMENT IN SPATIALLY UNCONSTRAINED MICROPHONE ARRAYS</div> <div class="author"> N. Furnon , R. Serizel , <em>S. Essid</em>, and I. Illina </div> <div class="periodical"> <em>IEEE/ACM Transactions on Audio, Speech and Language Processing</em>, Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hal.archives-ouvertes.fr/hal-02985867v3/file/furnon.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="cantisani:hal-03219350" class="col-sm-8"> <div class="title">USER-GUIDED ONE-SHOT DEEP MODEL ADAPTATION FOR MUSIC SOURCE SEPARATION</div> <div class="author"> g. Cantisani , A. Ozerov , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em> , Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hal.telecom-paris.fr/hal-03219350v3/file/UGOSA_Hal.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="furnon:hal-03259801" class="col-sm-8"> <div class="title">ATTENTION-BASED DISTRIBUTED SPEECH ENHANCEMENT FOR UNCONSTRAINED MICROPHONE ARRAYS WITH VARYING NUMBER OF NODES</div> <div class="author"> N. Furnon , R. Serizel , <em>S. Essid</em>, and I. Illina </div> <div class="periodical"> <em>In European Signal Processing Conference (EUSIPCO)</em> , Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hal.archives-ouvertes.fr/hal-03259801/file/eusipco2021.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="furnon:hal-02985794" class="col-sm-8"> <div class="title">DISTRIBUTED SPEECH SEPARATION IN SPATIALLY UNCONSTRAINED MICROPHONE ARRAYS</div> <div class="author"> N. Furnon , R. Serizel , I. Illina , and <em>S. Essid</em> </div> <div class="periodical"> <em>In ICASSP 2021 - 46th International Conference on Acoustics, Speech, and Signal Processing</em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hal.archives-ouvertes.fr/hal-02985794v2/file/icassp2021.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="cantisani:hal-02978978" class="col-sm-8"> <div class="title">NEURO-STEERED MUSIC SOURCE SEPARATION WITH EEG-BASED AUDITORY ATTENTION DECODING AND CONTRASTIVE-NMF</div> <div class="author"> G. Cantisani , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In ICASSP 2021 - 46th International Conference on Acoustics, Speech, and Signal Processing</em> , Jun 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://hal.telecom-paris.fr/hal-02978978v4/file/C-NMF-Hal.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p> We propose a novel informed music source separation paradigm, which can be referred to as neuro-steered music source separation. More precisely, the source separation process is guided by the user’s selective auditory attention decoded from his/her EEG response to the stimulus. This high-level prior information is used to select the desired instrument to isolate and to adapt the generic source separation model to the observed signal. To this aim, we leverage the fact that the attended instrument’s neural encoding is substantially stronger than the one of the unattended sources left in the mixture. This "contrast" is extracted using an attention decoder and used to inform a source separation model based on non-negative matrix fac-torization named Contrastive-NMF. The results are promising and show that the EEG information can automatically select the desired source to enhance and improve the separation quality. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="zaiem21_interspeech" class="col-sm-8"> <div class="title">CONDITIONAL INDEPENDENCE FOR PRETEXT TASK SELECTION IN SELF-SUPERVISED SPEECH REPRESENTATION LEARNING</div> <div class="author"> S. Zaiem , T. Parcollet , and <em>S. Essid</em> </div> <div class="periodical"> <em>In Interspeech</em> , Aug 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/zaiem21_interspeech.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:patent20" class="col-sm-8"> <div class="title">METHOD AND SYSTEM FOR BROADCASTING A MULTICHANNEL AUDIO STREAM TO TERMINALS OF SPECTATORS ATTENDING A SPORTS EVENT</div> <div class="author"> R. Blouet , and <em>S. Essid</em> </div> <div class="periodical"> <em>Patent Application</em>, Sep 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/US20210014627A1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="furnon:hal-02389159" class="col-sm-8"> <div class="title">DNN-BASED DISTRIBUTED MULTICHANNEL MASK ESTIMATION FOR SPEECH ENHANCEMENT IN MICROPHONE ARRAYS</div> <div class="author"> N. Furnon , R. Serizel , I. Illina , and <em>S. Essid</em> </div> <div class="periodical"> <em>In ICASSP 2020 - 45th International Conference on Acoustics, Speech, and Signal Processing</em> , May 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hal.archives-ouvertes.fr/hal-02389159/file/icassp2020.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="8926380" class="col-sm-8"> <div class="title">WEAKLY SUPERVISED REPRESENTATION LEARNING FOR AUDIO-VISUAL SCENE ANALYSIS</div> <div class="author"> S. Parekh , S. Essid , A. Ozerov , N. Duong , P. Pérez , and G. Richard </div> <div class="periodical"> <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, May 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="benyoussef:hal-02288044" class="col-sm-8"> <div class="title">ON-THE-FLY DETECTION OF USER ENGAGEMENT DECREASE IN SPONTANEOUS HUMAN-ROBOT INTERACTION</div> <div class="author"> A. Ben Youssef , G. Varni , <em>S. Essid</em>, and C. Clavel </div> <div class="periodical"> <em>International Journal of Social Robotics</em>, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="DBLP:journals/corr/abs-1902-10102" class="col-sm-8"> <div class="title">A MULTIMODAL MOVIE REVIEW CORPUS FOR FINE-GRAINED OPINION MINING</div> <div class="author"> A. Garcia , <em>S. Essid</em>, F. DAlche-Buc , and C. Clavel </div> <div class="periodical"> Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Garcia2019" class="col-sm-8"> <div class="title">FROM THE TOKEN TO THE REVIEW: A HIERARCHICAL MULTIMODAL APPROACH TO OPINION MINING</div> <div class="author"> A. Garcia , P. Colombo , F. DAlche-Buc , <em>S. Essid</em>, and C. Clavel </div> <div class="periodical"> <em>In 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing</em> , Nov 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Cantisani2019b" class="col-sm-8"> <div class="title">MAD-EEG: AN EEG DATASET FOR DECODING AUDITORY ATTENTION TO A TARGET INSTRUMENT IN POLYPHONIC MUSIC</div> <div class="author"> G. Cantisani , G. Tregoat , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In Speech, Music and Mind (SMM19), Satellite workshop of Interspeech 2019</em> , Nov 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Parekh2019" class="col-sm-8"> <div class="title">IDENTIFY, LOCATE AND SEPARATE: AUDIO-VISUAL OBJECT EXTRACTION IN LARGE VIDEO COLLECTIONS USING WEAK SUPERVISION</div> <div class="author"> S. Parekh , A. Ozerov , <em>S. Essid</em>, N. Duong , P. Perez , and G. Richard </div> <div class="periodical"> <em>In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em> , Oct 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Cantisani2019" class="col-sm-8"> <div class="title">EEG-BASED DECODING OF AUDITORY ATTENTION TO A TARGET INSTRUMENT IN POLYPHONIC MUSIC</div> <div class="author"> G. Cantisani , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em> , Oct 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Maia2019" class="col-sm-8"> <div class="title">SAMBASET: A DATASET OF HISTORICAL SAMBA DE ENREDO RECORDINGS FOR COMPUTATIONAL MUSIC ANALYSIS</div> <div class="author"> L. Maia , M. Fuentes , L. Biscainho , M. Rocamora , and <em>S. Essid</em> </div> <div class="periodical"> <em>In The 20th International Society for Music Information Retrieval Conference</em> , Nov 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Fuentes2019b" class="col-sm-8"> <div class="title">TRACKING BEATS AND MICROTIMING IN AFRO-LATIN AMERICAN MUSIC USING CONDITIONAL RANDOM FIELDS AND DEEP LEARNING</div> <div class="author"> M. Fuentes , L. Maia , M. Rocamora , L. Biscainho , H. Crayencour , <em>S. Essid</em>, and J. Bello </div> <div class="periodical"> <em>In The 20th International Society for Music Information Retrieval Conference</em> , Nov 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Fuentes2019" class="col-sm-8"> <div class="title">A MUSIC STRUCTURE INFORMED DOWNBEAT TRACKING SYSTEM USING SKIP-CHAIN CONDITIONAL RANDOM FIELDS AND DEEP LEARNING</div> <div class="author"> M. Fuentes , B. McFee , H. Crayencour , <em>S. Essid</em>, and J. Bello </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal processing</em> , May 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Duan2019" class="col-sm-8"> <div class="title">AUDIOVISUAL ANALYSIS OF MUSIC PERFORMANCES: OVERVIEW OF AN EMERGING FIELD</div> <div class="author"> Z. Duan , S. Essid , C. Liem , G. Richard , and G. Sharma </div> <div class="periodical"> <em>IEEE Signal Processing Magazine</em>, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="BenYoussef2019" class="col-sm-8"> <div class="title">EARLY DETECTION OF USER ENGAGEMENT BREAKDOWN IN SPONTANEOUS HUMAN-HUMANOID INTERACTION</div> <div class="author"> A. Ben Youssef , C. Clavel , and S. Essid </div> <div class="periodical"> <em>IEEE Transactions on Affective Computing</em>, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:patent18" class="col-sm-8"> <div class="title">PROCEDE ET SYSTEME DE DIFFUSION D UN FLUX AUDIO MULTICANAL A DES TERMINAUX DE SPECTATEURS ASSISTANT A UN EVENEMENT SPORTIF</div> <div class="author"> R. Blouet , and <em>S. Essid</em> </div> <div class="periodical"> <em>Patent Application</em>, Mar 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/FR3079706B1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="lostanlen_vincent_2018_1344103" class="col-sm-8"> <div class="title">MEDLEY-SOLOS-DB: A CROSS-COLLECTION DATASET FOR MUSICAL INSTRUMENT RECOGNITION</div> <div class="author"> V. Lostanlen , C. Cella , R. Bittner , and <em>S. Essid</em> </div> <div class="periodical"> Sep 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Hajlaoui2018" class="col-sm-8"> <div class="title">EEG-BASED INTER-SUBJECT CORRELATION SCHEMES IN A STIMULI-SHARED FRAMEWORK: INTERPLAY WITH VALENCE AND AROUSAL</div> <div class="author"> A. Hajlaoui , M. Chetouani , and <em>S. Essid</em> </div> <div class="periodical"> <em></em> Sep 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Fuentes2018" class="col-sm-8"> <div class="title">ANALYSIS OF COMMON DESIGN CHOICES IN DEEP LEARNING SYSTEMS FOR DOWNBEAT TRACKING</div> <div class="author"> M. Fuentes , B. McFee , H. Crayencour , <em>S. Essid</em>, and J. Bello </div> <div class="periodical"> <em>In Proceedings of the 19th International Society for Music Information Retrieval Conference</em> , Sep 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Basaran2018" class="col-sm-8"> <div class="title">MAIN MELODY ESTIMATION WITH SOURCE-FILTER NMF AND CRNN</div> <div class="author"> D. Basaran , <em>S. Essid</em>, and G. Peeters </div> <div class="periodical"> <em>In Proceedings of the 19th International Society for Music Information Retrieval Conference</em> , Sep 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Hong2018" class="col-sm-8"> <div class="title">A ROBUST AUDIO CLASSIFICATION SYSTEM FOR DETECTING PULMONARY EDEMA</div> <div class="author"> K. Hong , S. Essid , W. Ser , and D. Foo </div> <div class="periodical"> <em>Biomedical Signal Processing and Control</em>, Sep 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>In this paper we present a robust audio classification system to efficiently detect pulmonary edema. The system uses a feature learning technique based on (NMF), then classified with logistic regression. A study was done to compare feature engineering approaches with feature selection techniques against NMF. Different NMF schemes were investigated and also compared with Principal Component Analysis. NMF scored 95% F1 score, which was superior to feature engineering techniques that had scores from 83% to 93%. Background noise collected from hospitals and speech from a speech corpus database was used to simulate noisy data. The system was then tested using noisy data. The best NMF scheme scored 74%, while other feature engineering techniques scored lower; from 66% to 71%. NMF was also used as a signal enhancement tool. It improved the F1 score to 77%. Lastly, only inhalations from breath sounds were considered and this further improved classification results to 86%. The proposed robust classification system using NMF thus proved to be an effective method for audio-based detection of pulmonary edema. If implemented in real-time, the proposed system can be used as a screening tool.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Hajlaoui2019" class="col-sm-8"> <div class="title">MULTI-TASK FEATURE LEARNING FOR EEG-BASED EMOTION RECOGNITION USING GROUP NONNEGATIVE MATRIX FACTORIZATION</div> <div class="author"> A. Hajlaoui , M. Chetouani , and <em>S. Essid</em> </div> <div class="periodical"> <em>In The European Signal Processing Conference (EUSIPCO)</em> , Sep 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Garcia2018b" class="col-sm-8"> <div class="title">STRUCTURED OUTPUT LEARNING WITH ABSTENTION: APPLICATION TO ACCURATE OPINION PREDICTION</div> <div class="author"> A. Garcia , <em>S. Essid</em>, C. Clavel , and F. DAlche-Buc </div> <div class="periodical"> <em>In International Conference on Machine Learning (ICML)</em> , Jul 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Motivated by Supervised Opinion Analysis, we propose a novel framework devoted to Structured Output Learning with Abstention (SOLA). The structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a flexible way. For that purpose, we decompose the problem into the learning of a pair of predictors, one devoted to structured abstention and the other, to structured output prediction. To compare fully labeled training data with predictions potentially containing abstentions, we define a wide class of asymmetric abstention-aware losses. Learning is achieved by surrogate regression in an appropriate feature space while prediction with abstention is performed by solving a new pre-image problem. Thus, SOLA extends recent ideas about Structured Output Prediction via surrogate problems and calibration theory and enjoys statistical guarantees on the resulting excess risk. Instantiated on a hierarchical abstention-aware loss, SOLA is shown to be relevant for fine-grained opinion mining and gives state-of-the-art results on this task. Moreover, the abstention-aware representations can be used to competitively predict user-review ratings based on a sentence-level opinion predictor.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Parekh2018b" class="col-sm-8"> <div class="title">WEAKLY SUPERVISED REPRESENTATION LEARNING FOR UNSYNCHRONIZED AUDIO-VISUAL EVENTS</div> <div class="author"> S. Parekh , <em>S. Essid</em>, A. Ozerov , Q. Duong , P. Perez , and G. Richard </div> <div class="periodical"> <em>In CVPR Workshop on Sight and Sound (WSS)</em> , Jun 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Parekh2018" class="col-sm-8"> <div class="title">WEAKLY SUPERVISED REPRESENTATION LEARNING FOR UNSYNCHRONIZED AUDIO-VISUAL EVENTS</div> <div class="author"> S. Parekh , <em>S. Essid</em>, A. Ozerov , Q. Duong , P. Perez , and G. Richard </div> <div class="periodical"> Apr 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Audio-visual representation learning is an important task from the perspective of designing machines with the ability to understand complex events. To this end, we propose a novel multimodal framework that instantiates multiple instance learning. We show that the learnt representations are useful for classifying events and localizing their characteristic audio-visual elements. The system is trained using only video-level event labels without any timing information. An important feature of our method is its capacity to learn from unsynchronized audio-visual events. We achieve state-of-the-art results on a large-scale dataset of weakly-labeled audio event videos. Visualizations of localized visual regions and audio segments substantiate our system s efficacy, especially when dealing with noisy situations where modality-specific cues appear asynchronously.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Garcia2018" class="col-sm-8"> <div class="title">STRUCTURED OUTPUT LEARNING WITH ABSTENTION: APPLICATION TO ACCURATE OPINION PREDICTION</div> <div class="author"> A. Garcia , <em>S. Essid</em>, C. Clavel , and F. DAlche-Buc </div> <div class="periodical"> Mar 2018 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Motivated by Supervised Opinion Analysis, we propose a novel framework devoted to Structured Output Learning with Abstention (SOLA). The structure prediction model is able to abstain from predicting some labels in the structured output at a cost chosen by the user in a flexible way. For that purpose, we decompose the problem into the learning of a pair of predictors, one devoted to structured abstention and the other, to structured output prediction. To compare fully labeled training data with predictions potentially containing abstentions, we define a wide class of asymmetric abstention-aware losses. Learning is achieved by surrogate regression in an appropriate feature space while prediction with abstention is performed by solving a new pre-image problem. Thus, SOLA extends recent ideas about Structured Output Prediction via surrogate problems and calibration theory and enjoys statistical guarantees on the resulting excess risk. Instantiated on a hierarchical abstention-aware loss, SOLA is shown to be relevant for fine-grained opinion mining and gives state-of-the-art results on this task. Moreover, the abstention-aware representations can be used to competitively predict user-review ratings based on a sentence-level opinion predictor.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="schiratti:hal-01724272" class="col-sm-8"> <div class="title">AN ENSEMBLE LEARNING APPROACH TO DETECT EPILEPTIC SEIZURES FROM LONG INTRACRANIAL EEG RECORDINGS</div> <div class="author"> J. Schiratti , J. Le Douget , M. Le Van Quyen , <em>S. Essid</em>, and A. Gramfort </div> <div class="periodical"> <em>In International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> , Apr 2018 </div> <div class="periodical"> </div> <div class="links"> <a href="https://hal.archives-ouvertes.fr/hal-01724272/file/ICASSP.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="BARRIERE-18" class="col-sm-8"> <div class="title">ATTITUDE CLASSIFICATION IN ADJACENCY PAIRS OF A HUMAN-AGENT INTERACTION WITH HIDDEN CONDITIONAL RANDOM FIELDS</div> <div class="author"> V. Barriere , C. Clavel , and <em>S. Essid</em> </div> <div class="periodical"> <em>In International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> , Apr 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SP:brevet18" class="col-sm-8"> <div class="title">METHOD FOR AUDIO-VISUAL EVENTS CLASSIFICATION AND LOCALIZATION, AND CORRESPONDING APPARATUS, COMPUTER READABLE PROGRAM, PRODUCT AND COMPUTER READABLE STORAGE MEDIUM</div> <div class="author"> S. Parekh , <em>S. Essid</em>, A. Ozerov , Q. Duong , P. Perez , and G. Richard </div> <div class="periodical"> <em>Patent Application</em>, Apr 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ICML17" class="col-sm-8"> <div class="title">MATRIX CO-FACTORISATION AND APPLICATIONS TO MUSIC ANALYSIS</div> <div class="author"> <em>S. Essid</em> </div> <div class="periodical"> <em>In Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML) 2017</em> , Aug 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ICML-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="bisot:hal-01636627" class="col-sm-8"> <div class="title">NONNEGATIVE FEATURE LEARNING METHODS FOR ACOUSTIC SCENE CLASSIFICATION</div> <div class="author"> V. Bisot , R. Serizel , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In DCASE 2017 - Workshop on Detection and Classification of Acoustic Scenes and Events</em> , Nov 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/VB_DCASE-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SP:brevet17a" class="col-sm-8"> <div class="title">METHOD FOR PROCESSING AN INPUT AUDIO SIGNAL AND CORRESPONDING ELECTRONIC DEVICE</div> <div class="author"> S. Parekh , <em>S. Essid</em>, A. Ozerov , Q. Duong , P. Perez , and G. Richard </div> <div class="periodical"> <em>Patent Application</em>, Nov 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Serizel2017" class="col-sm-8"> <div class="title">COMPUTATIONAL ANALYSIS OF SOUND SCENES AND EVENTS</div> <div class="author"> R. Serizel , V. Bisot , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> Nov 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Essid2017" class="col-sm-8"> <div class="title">COMPUTATIONAL ANALYSIS OF SOUND SCENES AND EVENTS</div> <div class="author"> <em>S. Essid</em>, S. Parekh , Q. Duong , A. Ozerov , and R. Serizel </div> <div class="periodical"> Nov 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Benyoussef2017" class="col-sm-8"> <div class="title">UE-HRI: A NEW DATASET FOR THE STUDY OF USER ENGAGEMENT IN SPONTANEOUS HUMAN-ROBOT INTERACTIONS</div> <div class="author"> A. Ben Youssef , C. Clavel , <em>S. Essid</em>, M. Bilac , M. Chamoux , and A. Lim </div> <div class="periodical"> <em>In ACM International Conference on Multimodal Interaction</em> , Nov 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/AB_ICMI-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Bisot2017c" class="col-sm-8"> <div class="title">LEVERAGING DEEP NEURAL NETWORKS WITH NONNEGATIVE REPRESENTATIONS FOR IMPROVED ENVIRONMENTAL SOUND CLASSIFICATION</div> <div class="author"> V. Bisot , R. Serizel , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE International Workshop on Machine Learning for Signal Processing (MLSP)</em> , Sep 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/VB_MLSP-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Parekh2017b" class="col-sm-8"> <div class="title">GUIDING AUDIO SOURCE SEPARATION BY VIDEO OBJECT INFORMATION</div> <div class="author"> S. Parekh , <em>S. Essid</em>, A. Ozerov , Q. Duong , P. Perez , and G. Richard </div> <div class="periodical"> <em>In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em> , Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SP_WASPAA-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Conneau2017" class="col-sm-8"> <div class="title">EMOEEG: A NEW MULTIMODAL DATASET FOR DYNAMIC EEG-BASED EMOTION RECOGNITION WITH AUDIOVISUAL ELICITATION</div> <div class="author"> A. Conneau , A. Hajlaoui , M. Chetouani , and <em>S. Essid</em> </div> <div class="periodical"> <em>In The European Signal Processing Conference (EUSIPCO)</em> , Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/AC_EUSIPCO-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Barriere2017" class="col-sm-8"> <div class="title">OPINION DYNAMICS MODELING FOR MOVIE REVIEW TRANSCRIPTS</div> <div class="author"> V. Barriere , C. Clavel , and <em>S. Essid</em> </div> <div class="periodical"> <em>In Interspeech</em> , Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/VB_INTERSPEECH-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Bisot2017b" class="col-sm-8"> <div class="title">OVERLAPPING SOUND EVENT DETECTION WITH SUPERVISED NONNEGATIVE MATRIX FACTORIZATION</div> <div class="author"> V. Bisot , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/VB_ICASSP-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Bisot2017" class="col-sm-8"> <div class="title">FEATURE LEARNING WITH MATRIX FACTORIZATION APPLIED TO ACOUSTIC SCENE CLASSIFICATION</div> <div class="author"> V. Bisot , R. Serizel , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>IEEE Transactions on Audio, Speech, and Language Processing (TASLP)</em>, Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/VB_TASLP-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Serizel2018" class="col-sm-8"> <div class="title">SUPERVISED GROUP NONNEGATIVE MATRIX FACTORISATION WITH SIMILARITY CONSTRAINTS AND APPLICATIONS TO SPEAKER IDENTIFICATION</div> <div class="author"> R. Serizel , V. Bisot , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RS_ICASSP-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Parekh2017" class="col-sm-8"> <div class="title">MOTION INFORMED AUDIO SOURCE SEPARATION</div> <div class="author"> S. Parekh , <em>S. Essid</em>, A. Ozerov , Q. Duong , P. Perez , and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP)</em> , Oct 2017 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SP_ICASSP-17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:patent16" class="col-sm-8"> <div class="title">DISPOSITIF A CASQUE AUDIO PERFECTIONNE</div> <div class="author"> <em>S. Essid</em>, and R. Blouet </div> <div class="periodical"> <em>Patent Application</em>, Nov 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/FR3059191A1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Bisot2016b" class="col-sm-8"> <div class="title">SUPERVISED NONNEGATIVE MATRIX FACTORIZATION FOR ACOUSTIC SCENE CLASSIFICATION</div> <div class="author"> V. Bisot , R. Serizel , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE international evaluation campaign on detection and classification of acousitc scenes and events (DCASE 2016)</em> , Sep 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/VB_DCASE-16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SD:ISMIR-16" class="col-sm-8"> <div class="title">DOWNBEAT DETECTION WITH CONDITIONAL RANDOM FIELDS AND DEEP LEARNED FEATURES</div> <div class="author"> S. Durand , and <em>S. Essid</em> </div> <div class="periodical"> <em>In The 17th International Society for Music Information Retrieval Conference (ISMIR)</em> , Aug 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SD_ISMIR-16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="RS:MLSP-2016" class="col-sm-8"> <div class="title">MINI-BATCH STOCHASTIC APPROACHES FOR ACCELERATED MULTIPLICATIVE UPDATES IN NONNEGATIVE MATRIX FACTORISATION WITH BETA-DIVERGENCE</div> <div class="author"> R. Serizel , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE International Workshop on Machine Learning for Signal Processing (MLSP)</em> , Sep 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RS_MLSP-16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="RS:ICIP-2016" class="col-sm-8"> <div class="title">MACHINE LISTENING TECHNIQUES AS A COMPLEMENT TO VIDEO IMAGE ANALYSIS IN FORENSICS</div> <div class="author"> R. Serizel , V. Bisot , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In The International Conference on Image Processing (ICIP)</em> , Oct 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RS_ICIP-16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Bisot2016" class="col-sm-8"> <div class="title">ACOUSTIC SCENE CLASSIFICATION WITH MATRIX FACTORIZATION FOR UNSUPERVISED FEATURE LEARNING</div> <div class="author"> V. Bisot , R. Serizel , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/VB_ICASSP-16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Serizel2016a" class="col-sm-8"> <div class="title">GROUP NONNEGATIVE MATRIX FACTORISATION WITH SPEAKER AND SESSION VARIABILITY COMPENSATION FOR SPEAKER IDENTIFICATION</div> <div class="author"> R. Serizel , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Mar 2016 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RS_ICASSP-16.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:hdr" class="col-sm-8"> <div class="title">CONTRIBUTIONS IN MACHINE LEARNING FOR MULTIMODAL DATA ANALYSIS: METHODS, ALGORITHMS AND SYSTEMS FOR TEMPORALLY STRUCTURED DATA</div> <div class="author"> <em>S. Essid</em> </div> <div class="periodical"> <em>Université Pierre et Marie Curie</em> , Sep 2015 </div> <div class="periodical"> Habilitation Thesis </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Masurelle2015" class="col-sm-8"> <div class="title">TPT-DANCE&amp;ACTIONS : UN CORPUS MULTIMODAL D’ACTIVITES HUMAINES</div> <div class="author"> A. Masurelle , A. Sekkat , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>Revue Traitement du Signal</em>, Sep 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/AM_TS-15.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Bittner2015" class="col-sm-8"> <div class="title">MELODY EXTRACTION BY CONTOUR CLASSIFICATION</div> <div class="author"> R. Bittner , J. Salmon , <em>S. Essid</em>, and J. Bello </div> <div class="periodical"> <em>In International Conference on Music Information Retrieval (ISMIR)</em> , Sep 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RB_ISMIR-15.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Bisot2015" class="col-sm-8"> <div class="title">HOG AND SUBBAND POWER DISTRIBUTION IMAGE FEATURES FOR ACOUSTIC SCENE CLASSIFICATION</div> <div class="author"> V. Bisot , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In European Signal Processing Conference (EUSIPCO)</em> , Sep 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/VB_EUSIPCO-15.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="TF:ICASSP-15" class="col-sm-8"> <div class="title">A CONDITIONAL RANDOM FIELD SYSTEM FOR BEAT TRACKING</div> <div class="author"> T. Fillon , C. Joder , S. Durand , and <em>S. Essid</em> </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Apr 2015 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/TF_ICASSP-15.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Seichepine_Essid_Fevotte_Cappe_2014" class="col-sm-8"> <div class="title">SOFT NONNEGATIVE MATRIX CO-FACTORIZATION</div> <div class="author"> N. Seichepine , <em>S. Essid</em>, C. Fevotte , and O. Cappe </div> <div class="periodical"> <em>IEEE Transactions on Signal Processing</em>, Apr 2014 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/NS_TSP-14.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="NS:ICASSP-14" class="col-sm-8"> <div class="title">PIECEWISE CONSTANT NONNEGATIVE MATRIX FACTORIZATION</div> <div class="author"> N. Seichepine , <em>S. Essid</em>, C. Fevotte , and O. Cappe </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> , May 2014 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/NS_ICASSP-14.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="AC:ICASSP-14" class="col-sm-8"> <div class="title">ASSESSMENT OF NEW SPECTRAL FEATURES FOR EEG-BASED EMOTION RECOGNITION</div> <div class="author"> A. Conneau , and <em>S. Essid</em> </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> , May 2014 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/AC_ICASSP-14.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="AM:ICASSP-14" class="col-sm-8"> <div class="title">GESTURE RECOGNITION USING A NMF-BASED REPRESENTATION OF MOTION-TRACES EXTRACTED FROM DEPTH SILHOUETTES</div> <div class="author"> A. Masurelle , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> , May 2014 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/AM_ICASSP-14.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="gretsi-13" class="col-sm-8"> <div class="title">CO-FACTORISATION DOUCE EN MATRICES NON-NEGATIVES. APPLICATION AU REGROUPEMENT MULTIMODAL DE LOCUTEURS</div> <div class="author"> N. Seichepine , <em>S. Essid</em>, C. Fevotte , and O. Cappe </div> <div class="periodical"> <em>In GRETSI</em> , Sep 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CD:MLSP-13" class="col-sm-8"> <div class="title">NONNEGATIVE TENSOR FACTORIZATION FOR SINGLE-CHANNEL EEG ARTIFACT REJECTION</div> <div class="author"> C. Damon , A. Liutkus , A. Gramfort , and <em>S. Essid</em> </div> <div class="periodical"> <em>In IEEE International Workshop on Machine Learning for Signal Processing</em> , Sep 2013 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CD_MLSP-13.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="RF:Wiamis2013" class="col-sm-8"> <div class="title">EXPLORING NEW FEATURES FOR MUSIC CLASSIFICATION</div> <div class="author"> R. Foucard , <em>S. Essid</em>, G. Richard , and M. Lagrange </div> <div class="periodical"> <em>In International Workshop on Image and Audio Analysis for Multimedia Interactive Services (WIAMIS)</em> , Jul 2013 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RF_WIAMIS-13.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="AM:WIAMIS-13" class="col-sm-8"> <div class="title">MULTIMODAL CLASSIFICATION OF DANCE MOVEMENTS USING BODY JOINT TRAJECTORIES AND STEP SOUNDS</div> <div class="author"> A. Masurelle , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In International Workshop on Image and Audio Analysis for Multimedia Interactive Services (WIAMIS)</em> , Jul 2013 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/AM_WIAMIS-13.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Dremeau2013a" class="col-sm-8"> <div class="title">PROBABILISTIC DANCE PERFORMANCE ALIGNMENT BY FUSION OF MULTIMODAL FEATURES </div> <div class="author"> A. Dremeau , and <em>S. Essid</em> </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> , May 2013 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/AD_ICASSP-13.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="ICASSP-13-SEICHEPINE" class="col-sm-8"> <div class="title">SOFT NONNEGATIVE MATRIX CO-FACTORIZATION WITH APPLICATION TO MULTIMODAL SPEAKER DIARIZATION</div> <div class="author"> N. Seichepine , <em>S. Essid</em>, C. Fevotte , and O. Cappe </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em> , May 2013 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/NS_ICASSP-13.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CD:ICASSP-13" class="col-sm-8"> <div class="title">NONNEGATIVE MATRIX FACTORIZATION FOR SINGLE-CHANNEL EEG ARTIFACT REJECTION</div> <div class="author"> C. Damon , A. Liutkus , A. Gramfort , and <em>S. Essid</em> </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP) </em> , May 2013 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CD_ICASSP-13.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="FV:TM-12" class="col-sm-8"> <div class="title">A MULTIMODAL APPROACH TO SPEAKER DIARIZATION ON TV TALK-SHOWS</div> <div class="author"> F. Vallet , <em>S. Essid</em>, and J. Carrive </div> <div class="periodical"> <em>IEEE Transactions on Multimedia</em>, May 2013 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/research/assets/pdf/papers/FV_TM-13.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this article, we propose solutions to the problem of speaker diarization of TV talk-shows, a problem for which adapted multimodal approaches, relying on other streams of data than only audio, remain largely under exploited. Hence we propose an original system that leverages prior knowledge on the structure of this type of content, especially the visual information relating to the active speakers, for an improved diarization performance. The architecture of this system can be decomposed into two main stages. First a reliable training set is created, in an unsupervised fashion, for each participant of the TV program being processed. This data is assembled by the association of visual and audio descriptors carefully selected in a clustering cascade. Then, Support Vector Machines are used for the classification of the speech data (of a given TV program). The performance of this new architecture is assessed on two French talk-show collections: Le Grand Échiquier and On na pas tout dit. The results show that our new system outperforms state-of-the-art methods, thus evidencing the effectiveness of kernel-based methods, as well as visual cues, in multimodal approaches to speaker diarization of challenging contents such as TV talk-shows.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:TASLP-13" class="col-sm-8"> <div class="title">LEARNING OPTIMAL FEATURES FOR POLYPHONIC AUDIO-TO-SCORE ALIGNMENT</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, May 2013 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_TASLP-13.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:TMM-12" class="col-sm-8"> <div class="title">SMOOTH NONNEGATIVE MATRIX FACTORIZATION FOR UNSUPERVISED AUDIOVISUAL DOCUMENT STRUCTURING</div> <div class="author"> <em>S. Essid</em>, and C. Fevotte </div> <div class="periodical"> <em>IEEE Transactions on Multimedia</em>, May 2013 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_TM-12.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="AL:ACM-12" class="col-sm-8"> <div class="title">ANALYSIS OF DANCE MOVEMENTS USING GAUSSIAN PROCESSES</div> <div class="author"> A. Liutkus , A. Dremeau , D. Alexiadis , <em>S. Essid</em>, and P. Daras </div> <div class="periodical"> <em>In ACM Multimedia</em> , Nov 2012 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ICIP-12" class="col-sm-8"> <div class="title">DECOMPOSING THE VIDEO EDITING STRUCTURE OF A TALK-SHOW USING NONNEGATIVE MATRIX FACTORIZATION</div> <div class="author"> <em>S. Essid</em>, and C. Fevotte </div> <div class="periodical"> <em>In International Conference on Image Processing (ICIP)</em> , Oct 2012 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:DFU-12" class="col-sm-8"> <div class="title">MULTIMODAL MUSIC PROCESSING</div> <div class="author"> <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> Oct 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_DFU-12.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ICMI-12" class="col-sm-8"> <div class="title">A MULTI-MODAL DANCE CORPUS FOR RESEARCH INTO INTERACTION BETWEEN HUMANS IN VIRTUAL ENVIRONMENTS</div> <div class="author"> <em>S. Essid</em>, X. Lin , M. Gowing , G. Kordelas , A. Aksay , P. Kelly , T. Fillon , Q. Zhang , A. Dielmann , V. Kitanovski , R. Tournemenne , A. Masurelle , E. Izquierdo , N. OConnor , P. Daras , and G. Richard </div> <div class="periodical"> <em>Journal on Multimodal User Interfaces: Special issue on multimodal corpora</em>, Oct 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ICMI-12.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ICASSP-12b" class="col-sm-8"> <div class="title">AN ADVANCED VIRTUAL DANCE PERFORMANCE EVALUATOR</div> <div class="author"> <em>S. Essid</em>, D. Alexiadis , R. Tournemenne , M. Gowing , P. Kelly , D. Monhagan , P. Daras , A. Dremeau , and N. OConnor </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing</em> , Mar 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ICASSP-12.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ICASSP-12" class="col-sm-8"> <div class="title">A SINGLE-CLASS SVM BASED ALGORITHM FOR COMPUTING AN IDENTIFIABLE NMF</div> <div class="author"> <em>S. Essid</em> </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing</em> , Mar 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ICASSP-12b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="RF:ICASSP-12" class="col-sm-8"> <div class="title">A REGRESSIVE BOOSTING APPROACH TO AUTOMATIC AUDIO TAGGING BASED ON SOFT ANNOTATOR FUSION</div> <div class="author"> R. Foucard , <em>S. Essid</em>, M. Lagrange , and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Mar 2012 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RF_ICASSP-12.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2011</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ICMI-11" class="col-sm-8"> <div class="title">A MULTIMODAL DANCE CORPUS FOR RESEARCH INTO REAL-TIME INTERACTION BETWEEN HUMANS IN ONLINE VIRTUAL ENVIRONMENTS </div> <div class="author"> <em>S. Essid</em>, X. Lin , M. Gowing , G. Kordelas , A. Aksay , P. Kelly , T. Fillon , Q. Zhang , A. Dielmann , V. Kitanovski , R. Tournemenne , N. OConnor , P. Daras , and G. Richard </div> <div class="periodical"> <em>In ICMI Workshop On Multimodal Corpora For Machine Learning</em> , Nov 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ICMI-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ACM-MM-GC-2011" class="col-sm-8"> <div class="title">AN AUDIO-DRIVEN VIRTUAL DANCE-TEACHING ASSISTANT</div> <div class="author"> <em>S. Essid</em>, Y. Grenier , M. Maazaoui , G. Richard , and R. Tournemenne </div> <div class="periodical"> <em>In ACM Multimedia</em> , Nov 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ACM-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CC:ACM-MM-GC-2011" class="col-sm-8"> <div class="title">ENHANCED VISUALISATION OF DANCE PERFORMANCE FROM AUTOMATICALLY SYNCHRONISED MULTIMODAL RECORDINGS</div> <div class="author"> M. Gowing , P. Kelly , N. OConnor , E. Izquierdo , V. Kitanovski , X. Lin , Q. Zhang , C. Concolato , <em>S. Essid</em>, J. Feuvre , and R. Tournemenne </div> <div class="periodical"> <em>In ACM Multimedia</em> , Nov 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CC_ACM-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SG:ISMIR-11" class="col-sm-8"> <div class="title">AN INTERACTIVE SYSTEM FOR ELECTRO-ACOUSTIC MUSIC ANALYSIS</div> <div class="author"> S. Gulluni , <em>S. Essid</em>, O. Buisson , and G. Richard </div> <div class="periodical"> <em>In International Conference on Music Information Retrieval (ISMIR)</em> , Oct 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SG_ISMIR-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="RF:ISMIR-11" class="col-sm-8"> <div class="title">MULTI-SCALE TEMPORAL FUSION BY BOOSTING FOR MUSIC CLASSIFICATION</div> <div class="author"> R. Foucard , <em>S. Essid</em>, M. Lagrange , and G. Richard </div> <div class="periodical"> <em>In International Conference on Music Information Retrieval (ISMIR)</em> , Oct 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RF_ISMIR-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:WASPAA-11" class="col-sm-8"> <div class="title">OPTIMIZING THE MAPPING FROM A SYMBOLIC TO AN AUDIO REPRESENTATION FOR MUSIC-TO-SCORE ALIGNMENT</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</em> , Oct 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_WASPAA-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:TREP-11" class="col-sm-8"> <div class="title">NONNEGATIVE MATRIX FACTORIZATION FOR UNSUPERVISED AUDIOVISUAL DOCUMENT STRUCTURING</div> <div class="author"> <em>S. Essid</em>, and C. Fevotte </div> <div class="periodical"> Oct 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_HAL-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="GA:IFMGBOOK" class="col-sm-8"> <div class="title">SEMANTIQUE ET MULTIMODALITE EN ANALYSE DE L INFORMATION</div> <div class="author"> G. Adda , G. Chollet , <em>S. Essid</em>, T. Fillon , M. Garnier-Rizet , C. Hory , and L. Beltaifa-Zouari </div> <div class="periodical"> Oct 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="FV:TVBOOK-11" class="col-sm-8"> <div class="title">TV CONTENT ANALYSIS: TECHNIQUES AND APPLICATIONS</div> <div class="author"> F. Vallet , <em>S. Essid</em>, J. Carrive , and G. Richard </div> <div class="periodical"> Oct 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SG:AES-11" class="col-sm-8"> <div class="title">INTERACTIVE CLASSIFICATION OF SOUND OBJECTS FOR POLYPHONIC ELECTRO-ACOUSTIC MUSIC ANNOTATION</div> <div class="author"> S. Gulluni , <em>S. Essid</em>, O. Buisson , and G. Richard </div> <div class="periodical"> <em>In AES 42nd International Conference</em> , Jul 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SG_AES-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="RB:MMABOOK-11" class="col-sm-8"> <div class="title">MULTIMEDIA SEMANTICS: METADATA, ANALYSIS AND INTERACTION</div> <div class="author"> R. Benmokhtar , B. Huet , G. Richard , T. Declerck , and <em>S. Essid</em> </div> <div class="periodical"> Jul 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:MMABOOK-11" class="col-sm-8"> <div class="title">MULTIMEDIA SEMANTICS: METADATA, ANALYSIS AND INTERACTION</div> <div class="author"> <em>S. Essid</em>, M. Campedel , G. Richard , T. Piatrik , R. Benmokhtar , and B. Huet </div> <div class="periodical"> Jul 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:TSALP-2011" class="col-sm-8"> <div class="title">A CONDITIONAL RANDOM FIELD FRAMEWORK FOR ROBUST AND SCALABLE AUDIO-TO-SCORE MATCHING</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>IEEE Transactions on Audio, Speech and Language Processing</em>, Nov 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_TSALP-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:ICASSP-11" class="col-sm-8"> <div class="title">HIDDEN DISCRETE TEMPO MODEL: A TEMPO-AWARE TIMING MODEL FOR AUDIO-TO-SCORE ALIGNMENT</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , May 2011 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_ICASSP-11.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2010</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SB:Eusipco10" class="col-sm-8"> <div class="title">A MULTIMODAL APPROACH TO INITIALISATION FOR TOP-DOWN SPEAKER DIARIZATION OF TELEVISION SHOWS</div> <div class="author"> S. Bozonnet , F. Vallet , N. Evans , <em>S. Essid</em>, J. Carrive , and G. Richard </div> <div class="periodical"> <em>In European Signal Processing Conference (EUSIPCO)</em> , Aug 2010 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SB_EUSIPCO-10.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:ACM-2010" class="col-sm-8"> <div class="title">A CONDITIONAL RANDOM FIELD VIEWPOINT OF SYMBOLIC AUDIO-TO-SCORE MATCHING</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In ACM Multimedia 2010</em> , Oct 2010 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:CORESA-2010" class="col-sm-8"> <div class="title">APPROCHE HI&amp;EACUTE;RARCHIQUE POUR UN ALIGNEMENT MUSIQUE-SUR-PARTITION EFFICACE</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In Compression et Représentation des Signaux Audiovisuels (CORESA)</em> , Oct 2010 </div> <div class="periodical"> Received Young Researcher Award! </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_CORESA-2010.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:ICASSP-2010" class="col-sm-8"> <div class="title">A COMPARATIVE STUDY OF TONAL ACOUSTIC FEATURES FOR A SYMBOLIC LEVEL MUSIC-TO-SCORE ALIGNMENT</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Mar 2010 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_ICASSP-10.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:ISMIR-2010" class="col-sm-8"> <div class="title">AN IMPROVED HIERARCHICAL APPROACH FOR MUSIC-TO-SYMBOLIC SCORE ALIGNMENT</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In International Conference on Music Information Retrieval (ISMIR)</em> , Aug 2010 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_ISMIR-2010.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="BM:ISMIR10" class="col-sm-8"> <div class="title">YAAFE, AN EASY TO USE AND EFFICIENT AUDIO FEATURE EXTRACTION SOFTWARE</div> <div class="author"> B. Mathieu , <em>S. Essid</em>, T. Fillon , J. Prado , and G. Richard </div> <div class="periodical"> <em>In International Conference on Music Information Retrieval (ISMIR)</em> , Aug 2010 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/BM_ISMIR-10.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="FV:CORESA-10" class="col-sm-8"> <div class="title">DESCRIPTEURS VISUELS ROBUSTES POUR L IDENTIFICATION DE LOCUTEURS DANS DES EMISSIONS TELEVISEES DE TALK-SHOWS</div> <div class="author"> F. Vallet , <em>S. Essid</em>, J. Carrive , and G. Richard </div> <div class="periodical"> <em>In Compression et Représentation des Signaux Audiovisuels (CORESA)</em> , Oct 2010 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/FV_ICIP-10.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="FV:ICIP10" class="col-sm-8"> <div class="title">ROBUST VISUAL FEATURES FOR THE MULTIMODAL IDENTIFICATION OF UNREGISTERED SPEAKERS IN TV TALK-SHOWS</div> <div class="author"> F. Vallet , <em>S. Essid</em>, J. Carrive , and G. Richard </div> <div class="periodical"> <em>In IEEE International Conference on Image Processing (ICIP)</em> , Oct 2010 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/FV_ICIP-10.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2009</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SG:MML-09" class="col-sm-8"> <div class="title">INTERACTIVE SEGMENTATION OF ELECTRO-ACOUSTIC MUSIC</div> <div class="author"> S. Gulluni , <em>S. Essid</em>, O. Buisson , E. Favreau , and G. Richard </div> <div class="periodical"> <em>In 2nd International Workshop on Machine Learning and Music (MML - ECML - PKDD)</em> , Sep 2009 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SG_MML-09.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:GRETSI-09" class="col-sm-8"> <div class="title">ETUDE DES DESCRIPTEURS ACOUSTIQUES POUR L ALIGNEMENT TEMPOREL AUDIO-SUR-PARTITION MUSICALE</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In GRETSI</em> , Sep 2009 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_GRETSI-09.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ:TASLP-08" class="col-sm-8"> <div class="title">TEMPORAL INTEGRATION FOR AUDIO CLASSIFICATION WITH APPLICATION TO MUSICAL INSTRUMENT CLASSIFICATION</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>IEEE Transactions on Audio, Speech and Language Processing</em>, Jan 2009 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_TASLP-09.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="MaxL:ICASSP09" class="col-sm-8"> <div class="title">INCORPORATING PRIOR KNOWLEDGE ON THE DIGITAL MEDIA CREATION PROCESS INTO AUDIO CLASSIFIERS</div> <div class="author"> M. Lardeur , <em>S. Essid</em>, G. Richard , M. Haller , and T. Sikora </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Apr 2009 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/ML_ICASSP-09.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="AO:TREP-09" class="col-sm-8"> <div class="title">RECONNAISSANCE DES INSTRUMENTS DANS LA MUSIQUE POLYPHONIQUE PAR D&amp;EACUTE;COMPOSITION NMF ET CLASSIFICATION SVM</div> <div class="author"> A. Ozerov , <em>S. Essid</em>, and M. Charbit </div> <div class="periodical"> Apr 2009 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/AO_TREP-09.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2008</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ICIP-08" class="col-sm-8"> <div class="title">A COLLABORATIVE APPROACH TO AUTOMATIC RUSHES VIDEO SUMMARIZATION</div> <div class="author"> W. Bailer , E. Dumont , <em>S. Essid</em>, and B. Mérialdo </div> <div class="periodical"> <em>In IEEE ICIP Workshop on Multimedia Information Retrieval: New Trends and Challenges</em> , Oct 2008 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ICIP-08.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:SAMT-08" class="col-sm-8"> <div class="title">A COLLABORATIVE APPROACH TO VIDEO SUMMARIZATION</div> <div class="author"> E. Dumont , B. Merialdo , <em>S. Essid</em>, W. Bailer , D. Byrne , H. Bredin , N. OConnor , G. Jones , M. Haller , A. Krutz , T. Sikora , and T. Piatrik </div> <div class="periodical"> <em>In 3rd International Conference on Semantic and Digital Media Technologies (SAMT)</em> , Dec 2008 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_SAMT-08.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:TRECVID-08" class="col-sm-8"> <div class="title">RUSHES VIDEO SUMMARIZATION USING A COLLABORATIVE APPROACH</div> <div class="author"> E. Dumont , B. Merialdo , <em>S. Essid</em>, W. Bailer , H. Rehatschek , D. Byrne , H. Bredin , N. OConnor , G. Jones , A. Smeaton , . M. Haller , A. Krutz , T. Sikora , and T. Piatrik </div> <div class="periodical"> <em>In TRECVID 2008, ACM International Conference on Multimedia Information Retrieval 2008</em> , Nov 2008 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ACM-08.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="CJ-EUSIPCO-2008" class="col-sm-8"> <div class="title">ALIGNMENT KERNELS FOR AUDIO CLASSIFICATION WITH APPLICATION TO MUSIC INSTRUMENT RECOGNITION</div> <div class="author"> C. Joder , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In European Signal Processing Conference (EUSIPCO)</em> , Aug 2008 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/CJ_EUSIPCO-08.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SW:EUSIPCO08" class="col-sm-8"> <div class="title">ON THE ROBUSTNESS OF AUDIO FEATURES FOR MUSICAL INSTRUMENT CLASSIFICATION</div> <div class="author"> S. Wegener , M. Haller , J. Burred , T. Sikora , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>In European Signal Processing Conference (EUSIPCO)</em> , Sep 2008 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SW_EUSIPCO08.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2007</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="OG:CVST-07" class="col-sm-8"> <div class="title">ON THE CORRELATION OF AUTOMATIC AUDIO AND VISUAL SEGMENTATIONS OF MUSIC VIDEOS</div> <div class="author"> O. Gillet , <em>S. Essid</em>, and G. Richard </div> <div class="periodical"> <em>IEEE Transactions on Circuits and Systems for Video Technology</em>, Mar 2007 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/OG_TCSVT-07.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="ICA:07" class="col-sm-8"> <div class="title">TOWARDS POLYPHONIC MUSICAL INSTRUMENT RECOGNITION</div> <div class="author"> G. Richard , P. Leveau , L. Daudet , <em>S. Essid</em>, and B. David </div> <div class="periodical"> <em>In International Congress on Acoustics (ICA)</em> , Sep 2007 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/GR_ICA-07.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="GR:Icassp-07" class="col-sm-8"> <div class="title">COMBINED SUPERVISED AND UNSUPERVISED APPROACHES FOR AUTOMATIC SEGMENTATION OF RADIOPHONIC AUDIO STREAMS</div> <div class="author"> G. Richard , M. Ramona , and <em>S. Essid</em> </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Apr 2007 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/GR_ICASSP-07.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:TRECVID-07" class="col-sm-8"> <div class="title">K-SPACE AT TRECVID 2007</div> <div class="author"> P. Wilkins , T. Adamek , D. Byrne , G. Jones , H. Lee , G. Keenan , K. Guinness , N. OConnor , A. Smeaton , A. Amin , Z. Obrenovic , R. Benmokhtar , E. Galmar , B. Huet , <em>S. Essid</em>, R. Landais , F. Vallet , G. Papadopoulos , S. Vrochidis , V. Mezaris , I. Kompatsiaris , E. Spyrou , Y. Avrithis , R. Morzinger , P. Schallauer , W. Bailer , T. Piatrik , K. Chandramouli , E. Izquierdo , M. Haller , L. Goldmann , A. Samour , A. Cobet , T. Sikora , and P. Praks </div> <div class="periodical"> <em>In TRECVID 2007</em> , Nov 2007 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_TRECVID-07.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2006</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:COD-06" class="col-sm-8"> <div class="title">INSTRUMENT RECOGNITION IN POLYPHONIC MUSIC BASED ON AUTOMATIC TAXONOMIES</div> <div class="author"> <em>S. Essid</em>, G. Richard , and B. David </div> <div class="periodical"> <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, Jan 2006 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_TSALP-06b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:COD-06-2" class="col-sm-8"> <div class="title">MUSICAL INSTRUMENT RECOGNITION BY PAIRWISE CLASSIFICATION STRATEGIES</div> <div class="author"> <em>S. Essid</em>, G. Richard , and B. David </div> <div class="periodical"> <em>IEEE Transactions on Audio, Speech, and Language Processing</em>, Jul 2006 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_TSALP-06a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ICASSP-06" class="col-sm-8"> <div class="title">HIERARCHICAL CLASSIFICATION OF MUSICAL INSTRUMENTS ON SOLO RECORDINGS</div> <div class="author"> <em>S. Essid</em>, G. Richard , and B. David </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , May 2006 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ICASSP-06.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2005</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:these" class="col-sm-8"> <div class="title">CLASSIFICATION AUTOMATIQUE DES SIGNAUX AUDIO-FR&amp;EACUTE;QUENCES: RECONNAISSANCE DES INSTRUMENTS DE MUSIQUE</div> <div class="author"> <em>S. Essid</em> </div> <div class="periodical"> <em>Université Pierre et Marie Curie</em> , Dec 2005 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_PhD-05.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE-ICASSP-05" class="col-sm-8"> <div class="title">INSTRUMENT RECOGNITION IN POLYPHONIC MUSIC</div> <div class="author"> <em>S. Essid</em>, G. Richard , and B. David </div> <div class="periodical"> <em>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> , Mar 2005 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ICASSP-05.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="PL-AES-05" class="col-sm-8"> <div class="title">ON THE USEFULNESS OF DIFFERENTIATED TRANSIENT/STEADY-STATE PROCESSING IN MACHINE RECOGNITION OF MUSICAL INSTRUMENTS</div> <div class="author"> P. Leveau , <em>S. Essid</em>, G. Richard , L. Daudet , and B. David </div> <div class="periodical"> <em>In AES convention</em> , May 2005 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_AES-05.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2004</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:AES-04" class="col-sm-8"> <div class="title">EFFICIENT MUSICAL INSTRUMENT RECOGNITION ON SOLO PERFORMANCES USING BASIC FEATURES</div> <div class="author"> <em>S. Essid</em>, G. Richard , and B. David </div> <div class="periodical"> <em>In AES 25th conference</em> , Jun 2004 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_AES-04.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:Eusipco-04" class="col-sm-8"> <div class="title">MUSICAL INSTRUMENT RECOGNITION ON SOLO PERFORMANCES</div> <div class="author"> <em>S. Essid</em>, G. Richard , and B. David </div> <div class="periodical"> <em>In European Signal Processing Conference (EUSIPCO)</em> , Sep 2004 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_AES-04.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:ISMIR-04" class="col-sm-8"> <div class="title">MUSICAL INSTRUMENT RECOGNITION BASED ON CLASS PAIRWISE FEATURE SELECTION</div> <div class="author"> <em>S. Essid</em>, G. Richard , and B. David </div> <div class="periodical"> <em>In International Conference on Music Information Retrieval (ISMIR)</em> , Oct 2004 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_ISMIR-04.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2003</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:GRETSI-03" class="col-sm-8"> <div class="title">MOD&amp;EGRAVE;LES SINUSO&amp;IUML;DAUX &amp;EACUTE;TENDUS POUR LE CODAGE AUDIO</div> <div class="author"> R. Boyer , <em>S. Essid</em>, K. Abed-Meraim , and N. Moreau </div> <div class="periodical"> <em>In Dix-neuvième colloque sur le Traitement du Signal et des Images</em> , Sep 2003 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RB_GRETSI-03.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li></ol> <h2 class="bibliography">2002</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="RB:DSP-02" class="col-sm-8"> <div class="title">TRANSIENT MODELING WITH A FREQUENCY-TRANSFORM SUBSPACE ALGORITHM AND TRANSIENT + SINUSOIDAL SCHEME</div> <div class="author"> R. Boyer , and <em>S. Essid</em> </div> <div class="periodical"> <em>In 14th IEEE Int. Conf. on Digital Signal Proc.</em> , Jul 2002 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/research/assets/pdf/papers/RB_DSP-02.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We present an efficient modeling method for strong transient character audio signals. It is shown that the parametric non-stationary exponentially damped sinusoids (EDS) model permits good performance for time domain modeling of quasi-stationary signals or "weak" transients. However, a decay in modeling performance is observed when dealing with highly nonstationary signals as in a variety of musical sounds (various percussions, castanets, triangle,...). The idea is then to process the signal in a well chosen frequency-transform domain in which the transient temporal characteristics are better modeled by EDS. As a result, better representations of the transient signal class are obtained with no pre-echo artifacts (energy before the attack) and a very good signal onset dynamic reproduction. Finally, an original "transient+sinusoidal" modeling scheme is proposed.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="RB:ICM-02" class="col-sm-8"> <div class="title">DYNAMIC TEMPORAL SEGMENTATION IN PARAMETRIC NON-STATIONARY MODELING FOR PERCUSSIVE MUSICAL SIGNALS</div> <div class="author"> R. Boyer , <em>S. Essid</em>, and N. Moreau </div> <div class="periodical"> <em>In IEEE Int. Conf. on Multimedia and Expo (ICME)</em> , Aug 2002 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/research/assets/pdf/papers/RB_ICM-02.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>An audio signal parametric modeling scheme is proposed that permits higher performance for representing strong sound transients. The exponentially damped sinusoids (EDS) model is considered in association with a high resolution parameter estimation approach. Such a technique is well adapted to almost every audio signal but is unfortunately not efficient when dealing with signals presenting strong temporal variations, such as percussive music signals, and causes pre-echo artifacts and weak onset dynamic reproduction which are prejudicial to listening. A system, based on the EDS model, has been developed with a transient detector and dynamic time segmentation and modeling that allows to overcome such artifacts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="RB:ICS-02" class="col-sm-8"> <div class="title">NON-STATIONARY SIGNAL PARAMETRIC MODELING TECHNIQUES WITH AN APPLICATION TO LOW BITRATE AUDIO CODING</div> <div class="author"> R. Boyer , <em>S. Essid</em>, and N. Moreau </div> <div class="periodical"> <em>In 6th IEEE Int. Conf. Signal Processing</em> , Aug 2002 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/research/assets/pdf/papers/RB_ICS-02.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Low bit rate audio coding often relies on Fourier representation despite its limitations for transient signal modeling. This study proposes alternative decompositions and expansion strategies that lead to more accurate modeling. Two classes of methods are considered, subspace decomposition methods, and atomic decomposition methods and their performances are compiled to propose an audio modeling scheme amenable to low bit rate coding.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="SE:master-02" class="col-sm-8"> <div class="title">CODEUR AUDIO PARAM&amp;EACUTE;TRIQUE BAS D&amp;EACUTE;BIT BAS&amp;EACUTE; SUR UN MOD&amp;EGRAVE;LE "SINUSO&amp;IUML;DES AMORTIES EXPONENTIELLEMENT + TRANSITOIRES + BRUIT"</div> <div class="author"> <em>S. Essid</em> </div> <div class="periodical"> <em>Ecole Nationale Supérieure des Télécommunications (ENST)</em> , Oct 2002 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/SE_MastTh-02.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2001</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="Boyer_01" class="col-sm-8"> <div class="title">EXPLORATION DE TECHNIQUES MODERNES DE MOD&amp;EACUTE;LISATION ADAPT&amp;EACUTE;ES &amp;AGRAVE; DU CODAGE AUDIO BAS-D&amp;EACUTE;BIT</div> <div class="author"> R. Boyer , <em>S. Essid</em>, and N. Moreau </div> <div class="periodical"> <em>In 7èmes Journées d Etudes et d Echanges : Compression et Représentation des Signaux Audiovisuels (CORESA)</em> , Oct 2001 </div> <div class="periodical"> </div> <div class="links"> <a href="/research/assets/pdf/papers/RB_CORESA-01.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Slim Essid. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/research/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/research/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/research/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/research/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/research/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/research/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>